{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4be5f5e",
   "metadata": {},
   "source": [
    "Question 1: What is a Decision Tree, and how does it work in the context of classification? \n",
    "\n",
    "Ans: A Decision Tree is a non-parametric supervised learning algorithm used for both classification and regression tasks. It has a hierarchical, tree-like structure consisting of a root node, branches, internal nodes, and leaf nodes. \n",
    "\n",
    "How it works in classification:\n",
    "\n",
    "1. Root Node: The process starts at the top with the entire dataset.\n",
    "2. Splitting: The algorithm selects the best feature to split the data into subsets that are more \"pure\" (containing instances of the same class).\n",
    "3. Internal Nodes: These represent decision points based on specific feature values (e.g., \"Is age > 30?\").\n",
    "4. Leaf Nodes: These represent the final classification category. Once a data point reaches a leaf node, it is assigned the majority label of that node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d29c376",
   "metadata": {},
   "source": [
    "Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree? \n",
    "\n",
    "Ans: Both measures quantify the \"disorder\" or \"impurity\" of a node. \n",
    "\n",
    "* Gini Impurity: Measures the probability of a randomly chosen element being incorrectly classified if it was randomly labeled according to the distribution of labels in the node. It ranges from 0 (pure) to 0.5 (equally distributed).\n",
    "\n",
    "\n",
    "* Entropy: Derived from information theory, it measures the level of uncertainty or randomness in the data. It ranges from 0 (pure) to 1 (equally distributed).\n",
    "\n",
    "Impact on Splits:\n",
    "The algorithm calculates the Gini or Entropy for every possible split. It chooses the split that results in the highest reduction in impurity (Information Gain).  Gini is often preferred as it is computationally faster because it doesn't involve logarithmic calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dcf4d0",
   "metadata": {},
   "source": [
    "Question 3: Difference between Pre-Pruning and Post-Pruning. Give one practical advantage of each. \n",
    "\n",
    "Ans: Pruning is used to prevent overfitting by simplifying the tree. \n",
    "\n",
    "| Feature | Pre-Pruning (Early Stopping) | Post-Pruning (Cost Complexity Pruning) |\n",
    "| --- | --- | --- |\n",
    "| Method | Stops the tree from growing once it hits certain criteria (e.g., `max_depth`). | Allows the tree to grow fully, then removes branches that provide little predictive power. |\n",
    "| Advantage | Saves computational time and memory by not building a massive tree. | More accurate model because it considers the whole tree before deciding what is unimportant. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae79878c",
   "metadata": {},
   "source": [
    "Question 4: What is Information Gain and why is it important for choosing the best split? \n",
    "\n",
    "Ans: Information Gain is the difference between the entropy of the parent node and the weighted average entropy of the child nodes.   \n",
    "Importance: It serves as the selection criterion. The feature that provides the \"highest Information Gain\" is considered the most informative feature for that specific split. By maximizing Information Gain at each step, the tree becomes as shallow and efficient as possible while maintaining high accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a86b45",
   "metadata": {},
   "source": [
    "Question 5: Real-world applications, advantages, and limitations. \n",
    "\n",
    "Ans: Applications: Credit scoring (loan default prediction), medical diagnosis, and customer churn prediction in marketing.   \n",
    "Advantages:\n",
    "* Easy to understand and visualize (White-box model).\n",
    "* Requires little data preprocessing (no need for scaling/normalization).\n",
    "* Handles both numerical and categorical data.   \n",
    "Limitations:\n",
    "* High tendency to overfit (captures noise in data).\n",
    "* Unstable: Small changes in data can lead to a completely different tree structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d3de16",
   "metadata": {},
   "source": [
    "Question 6: Write a Python program to:\n",
    "● Load the Iris Dataset\n",
    "● Train a Decision Tree Classifier using the Gini criterion\n",
    "● Print the model’s accuracy and feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07a442af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000\n",
      "Feature Importances: [0.         0.01667014 0.90614339 0.07718647]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"Feature Importances:\", clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71d07ff",
   "metadata": {},
   "source": [
    "Question 7: Write a Python program to:\n",
    "● Load the Iris Dataset\n",
    "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
    "a fully-grown tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d21c497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (max_depth=3): 1.0000\n",
      "Accuracy (Fully Grown): 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=42)\n",
    "\n",
    "# Max Depth = 3\n",
    "clf_3 = DecisionTreeClassifier(max_depth=3, random_state=42).fit(X_train, y_train)\n",
    "acc_3 = accuracy_score(y_test, clf_3.predict(X_test))\n",
    "\n",
    "clf_full = DecisionTreeClassifier(random_state=42).fit(X_train, y_train)\n",
    "acc_full = accuracy_score(y_test, clf_full.predict(X_test))\n",
    "\n",
    "print(f\"Accuracy (max_depth=3): {acc_3:.4f}\")\n",
    "print(f\"Accuracy (Fully Grown): {acc_full:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df61aef",
   "metadata": {},
   "source": [
    "Question 8: Write a Python program to:\n",
    "● Load the Boston Housing Dataset\n",
    "● Train a Decision Tree Regressor\n",
    "● Print the Mean Squared Error (MSE) and feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3bd604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:10: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:10: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\Tatha\\AppData\\Local\\Temp\\ipykernel_25400\\3806780019.py:10: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 10.4161\n",
      "Feature Importances: [5.12956739e-02 3.35270585e-03 5.81619171e-03 2.27940651e-06\n",
      " 2.71483790e-02 6.00326256e-01 1.36170630e-02 7.06881622e-02\n",
      " 1.94062297e-03 1.24638653e-02 1.10116089e-02 9.00872742e-03\n",
      " 1.93328464e-01]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Loading from a URL (standard alternative to the deprecated library)\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "X = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "y = raw_df.values[1::2, 2]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "regressor = DecisionTreeRegressor(random_state=42)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "y_pred = regressor.predict(X_test)\n",
    "print(f\"Mean Squared Error (MSE): {mean_squared_error(y_test, y_pred):.4f}\")\n",
    "print(\"Feature Importances:\", regressor.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6f59c5",
   "metadata": {},
   "source": [
    "Question 9: Write a Python program to:\n",
    "● Load the Iris Dataset\n",
    "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
    "GridSearchCV\n",
    "● Print the best parameters and the resulting model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5349a2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 3, 'min_samples_split': 2}\n",
      "Best Accuracy: 0.9733\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "param_grid = {'max_depth': [2, 3, 4, 5], 'min_samples_split': [2, 5, 10]}\n",
    "\n",
    "grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5)\n",
    "grid_search.fit(iris.data, iris.target)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(f\"Best Accuracy: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b20075",
   "metadata": {},
   "source": [
    "Question 10: Imagine you’re working as a data scientist for a healthcare company that\n",
    "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
    "mixed data types and some missing values.\n",
    "Explain the step-by-step process you would follow to:\n",
    "● Handle the missing values\n",
    "● Encode the categorical features\n",
    "● Train a Decision Tree model\n",
    "● Tune its hyperparameters\n",
    "● Evaluate its performance\n",
    "And describe what business value this model could provide in the real-world\n",
    "setting.\n",
    "\n",
    "Ans: \n",
    "Process:\n",
    "\n",
    "1. Handle Missing Values: Use Imputation. For numerical data (like age), use the mean/median. For categorical data (like blood type), use the mode or a \"Missing\" flag. \n",
    "2. Encode Categorical Features: Use One-Hot Encoding for nominal data (e.g., Gender) or Label Encoding for ordinal data (e.g., Disease Stage: Mild, Moderate, Severe). \n",
    "3. Train Model: Split the data (80/20) and train a `DecisionTreeClassifier`. \n",
    "4. Tune Hyperparameters: Use `GridSearchCV` to find the optimal `max_depth` and `min_samples_leaf` to prevent overfitting to patient noise. \n",
    "5. Evaluate Performance: Use a Confusion Matrix, focusing on Recall (to minimize false negatives, as missing a disease diagnosis is critical). \n",
    "\n",
    "Business Value: This model provides Decision Support. It allows doctors to prioritize high-risk patients for early intervention, leading to better patient outcomes and reduced long-term hospital costs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdac-class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
