{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45a8c67e",
   "metadata": {},
   "source": [
    "1. What is Simple Linear Regression?\n",
    "\n",
    "Ans:  \n",
    "\n",
    "Simple Linear Regression is a statistical method used to model the linear relationship between a single independent variable (predictor) and a dependent variable (response)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cd2f64",
   "metadata": {},
   "source": [
    "2. What are the key assumptions of Simple Linear Regression?\n",
    "\n",
    "    Linearity: The relationship between the independent and dependent variables is linear.\n",
    "\n",
    "    Independence of Errors: The residuals (errors) are independent of each other.\n",
    "\n",
    "    Homoscedasticity (Constant Variance): The variance of the residuals is constant across all levels of the independent variable.\n",
    "\n",
    "    Normality of Errors: The residuals are normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8831df93",
   "metadata": {},
   "source": [
    "3. What does the coefficient m represent in the equation Y=mX+c?\n",
    "\n",
    "    The coefficient m represents the slope of the regression line. It indicates the change in the dependent variable (Y) for a one-unit increase in the independent variable (X)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c33091",
   "metadata": {},
   "source": [
    "4. What does the intercept c represent in the equation Y=mX+c?\n",
    "\n",
    "    The intercept c represents the value of the dependent variable (Y) when the independent variable (X) is zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9421a57",
   "metadata": {},
   "source": [
    "\n",
    "5. How do we calculate the slope m in Simple Linear Regression?\n",
    "\n",
    "    The slope m is calculated using the following formula: m=∑i=1n​(Xi​−X')2∑i=1n​(Xi​−X')(Yi​−Y')​.\n",
    "\n",
    "    Alternatively, m=rSx​Sy​​, where r is the correlation coefficient, Sy​ is the standard deviation of Y, and Sx​ is the standard deviation of X."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95e3673",
   "metadata": {},
   "source": [
    "\n",
    "6. What is the purpose of the least squares method in Simple Linear Regression?\n",
    "\n",
    "    The purpose of the least squares method is to find the line that minimizes the sum of the squared differences (residuals) between the observed dependent variable values and the values predicted by the linear model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514b3c8c",
   "metadata": {},
   "source": [
    "\n",
    "7. How is the coefficient of determination (R2) interpreted in Simple Linear Regression?\n",
    "\n",
    "    R2 is interpreted as the proportion of the variance in the dependent variable that is predictable from the independent variable(s). A value of 0.75, for example, means 75% of the variance in Y is explained by X.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4ebb8c",
   "metadata": {},
   "source": [
    "\n",
    "8. What is Multiple Linear Regression?\n",
    "\n",
    "    Multiple Linear Regression is a statistical method used to model the linear relationship between two or more independent variables and a single dependent variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19933777",
   "metadata": {},
   "source": [
    "\n",
    "9. What is the main difference between Simple and Multiple Linear Regression?\n",
    "\n",
    "    Simple Linear Regression involves only one independent variable.\n",
    "\n",
    "    Multiple Linear Regression involves two or more independent variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8948a16b",
   "metadata": {},
   "source": [
    "\n",
    "10. What are the key assumptions of Multiple Linear Regression?\n",
    "\n",
    "    Linearity: The dependent variable is linearly related to the independent variables.\n",
    "\n",
    "    Independence of Errors: Residuals are independent.\n",
    "\n",
    "    Homoscedasticity (Constant Variance): Residuals have constant variance.\n",
    "\n",
    "    Normality of Errors: Residuals are normally distributed.\n",
    "\n",
    "    No Multicollinearity: The independent variables are not highly correlated with each other.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a46ebae",
   "metadata": {},
   "source": [
    "\n",
    "11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
    "\n",
    "    Heteroscedasticity is the condition where the variance of the residuals is not constant across all levels of the independent variables.\n",
    "\n",
    "    It does not bias the regression coefficients, but it makes the standard errors and p-values unreliable, leading to potentially incorrect hypothesis tests and confidence intervals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfa11d2",
   "metadata": {},
   "source": [
    "\n",
    "12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
    "\n",
    "    Remove one or more highly correlated independent variables.\n",
    "\n",
    "    Combine highly correlated variables into a single composite variable.\n",
    "\n",
    "    Use regularization techniques like Ridge or Lasso regression.\n",
    "\n",
    "    Collect more data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9deeb9d",
   "metadata": {},
   "source": [
    "\n",
    "13. What are some common techniques for transforming categorical variables for use in regression models?\n",
    "\n",
    "    One-Hot Encoding: Creates a new binary (0 or 1) dummy variable for each category.\n",
    "\n",
    "    Dummy Coding (or k-1 Coding): Similar to One-Hot Encoding but omits one category to serve as a reference group, avoiding the dummy variable trap (perfect multicollinearity).\n",
    "\n",
    "    Label Encoding: Assigns a unique integer to each category, typically only suitable for ordinal categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8733e07",
   "metadata": {},
   "source": [
    "\n",
    "14. What is the role of interaction terms in Multiple Linear Regression?\n",
    "\n",
    "    Interaction terms allow the effect of one independent variable on the dependent variable to vary depending on the level of another independent variable. They capture synergy or antagonism between predictors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ee2fdf",
   "metadata": {},
   "source": [
    "\n",
    "15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
    "\n",
    "    In Simple Linear Regression, the intercept is the predicted Y when the single X is zero.\n",
    "\n",
    "    In Multiple Linear Regression, the intercept is the predicted Y when all independent variables (X's) are zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492ab43f",
   "metadata": {},
   "source": [
    "\n",
    "16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
    "\n",
    "    The slope is significant as it quantifies the magnitude and direction of the relationship between the predictor(s) and the response.\n",
    "\n",
    "    It directly affects predictions by determining how much the predicted response changes for a unit change in the predictor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e2c5ad",
   "metadata": {},
   "source": [
    "\n",
    "17. How does the intercept in a regression model provide context for the relationship between variables?\n",
    "\n",
    "    The intercept provides context by establishing the baseline or starting point for the prediction. It represents the predicted value of the dependent variable when all predictors have a value of zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6be2d3",
   "metadata": {},
   "source": [
    "\n",
    "18. What are the limitations of using R2 as a sole measure of model performance?\n",
    "\n",
    "    R2 always increases or stays the same when more independent variables are added, even if those variables are not significant. It does not account for the number of predictors.\n",
    "\n",
    "    A high R2 does not guarantee that the model assumptions are met or that the model is the best fit for the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493102b6",
   "metadata": {},
   "source": [
    "\n",
    "19. How would you interpret a large standard error for a regression coefficient?\n",
    "\n",
    "    A large standard error indicates that the estimate of the regression coefficient is less precise or less reliable. It suggests that the coefficient's true value is likely to be far from the estimated value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ebfb5d",
   "metadata": {},
   "source": [
    "\n",
    "20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
    "\n",
    "    In a residual plot (residuals vs. predicted values), heteroscedasticity is identified by a non-random, distinct pattern in the spread of residuals, such as a funnel shape.\n",
    "\n",
    "    It is important to address it because it leads to incorrect standard errors and p-values, compromising the validity of hypothesis tests and confidence intervals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f393db",
   "metadata": {},
   "source": [
    "\n",
    "21. What does it mean if a Multiple Linear Regression model has a high R2 but low adjusted R2?\n",
    "\n",
    "    It means the model contains too many non-significant independent variables. The high R2 is inflated by the sheer number of predictors, but the low adjusted R2 penalizes the model for including predictors that do not significantly improve the fit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b4a59e",
   "metadata": {},
   "source": [
    "\n",
    "22. Why is it important to scale variables in Multiple Linear Regression?\n",
    "\n",
    "    Scaling variables ensures that all independent variables contribute equally to the model fitting process, regardless of their original units or magnitude.\n",
    "\n",
    "    It is particularly important for models that use regularization techniques (like Ridge or Lasso) and for comparing the relative importance of coefficients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00eec858",
   "metadata": {},
   "source": [
    "\n",
    "23. What is polynomial regression?\n",
    "\n",
    "    Polynomial regression is a form of regression analysis where the relationship between the independent variable (X) and the dependent variable (Y) is modeled as an nth-degree polynomial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8820e3b7",
   "metadata": {},
   "source": [
    "\n",
    "24. How does polynomial regression differ from linear regression?\n",
    "\n",
    "    Linear Regression models the relationship using a straight line (Y=mX+c).\n",
    "\n",
    "    Polynomial Regression models the relationship using a curved line based on polynomial terms (e.g., X2,X3), allowing it to fit non-linear data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df71711c",
   "metadata": {},
   "source": [
    "\n",
    "25. When is polynomial regression used?\n",
    "\n",
    "    It is used when the relationship between the variables is clearly non-linear and the simple linear model does not provide an adequate fit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbc9ae7",
   "metadata": {},
   "source": [
    "\n",
    "26. What is the general equation for polynomial regression?\n",
    "\n",
    "    The general equation for a polynomial regression model of degree k is:\n",
    "    Y=c+m1​X+m2​X2+m3​X3+...+mk​Xk+ϵ\n",
    "\n",
    "    where c is the intercept, mi​ are the coefficients, and ϵ is the error term.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af8c4c8",
   "metadata": {},
   "source": [
    "\n",
    "27. Can polynomial regression be applied to multiple variables?\n",
    "\n",
    "    Yes, polynomial regression can be extended to multiple independent variables. This is called Multivariate Polynomial Regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb7d7a3",
   "metadata": {},
   "source": [
    "\n",
    "28. What are the limitations of polynomial regression?\n",
    "\n",
    "    It is highly susceptible to overfitting, especially with high-degree polynomials, which can fit the training data very closely but perform poorly on new data.\n",
    "\n",
    "    It can be sensitive to outliers.\n",
    "\n",
    "    The high-degree terms can make the model less interpretable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfbc4e4",
   "metadata": {},
   "source": [
    "\n",
    "29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
    "\n",
    "    Cross-validation (e.g., K-fold) is used to estimate the model's performance on unseen data and identify overfitting.\n",
    "\n",
    "    Adjusted R2 can be used, as it penalizes the inclusion of unnecessary terms.\n",
    "\n",
    "    AIC (Akaike Information Criterion) or BIC (Bayesian Information Criterion) are used to balance model fit and complexity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb7d52d",
   "metadata": {},
   "source": [
    "\n",
    "30. Why is visualization important in polynomial regression?\n",
    "\n",
    "    Visualization is important because it allows for the initial assessment of the non-linear relationship between variables.\n",
    "\n",
    "    It is also crucial for detecting overfitting, as a highly complex curve that perfectly fits training data but looks erratic suggests a high degree of polynomial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbf0398",
   "metadata": {},
   "source": [
    "\n",
    "31. How is polynomial regression implemented in Python?\n",
    "\n",
    "    It is typically implemented in Python using the PolynomialFeatures class from the sklearn.preprocessing module to transform the independent variable(s) into a polynomial feature matrix.\n",
    "\n",
    "    A standard LinearRegression model from sklearn.linear_model is then fitted to the new polynomial feature matrix."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qml_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
